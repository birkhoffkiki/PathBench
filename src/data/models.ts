import { Model } from "@/types";
import { FaGithub, FaJournalWhills,  } from 'react-icons/fa';
import {SiArxiv, SiHuggingface, SiPytorch  } from "react-icons/si";


export const models: Model[] = [
{
    id: 'ResNet50',
    name: 'ResNet50',
    parameters: 8.5,
    dim: 1024,
    trainingData: 'ImageNet',
    description: 'pretrained on image net, the last conv layer is dropped to obtain the feature with dimension of 1024',
    architecture: 'ResNet50',
    modelType: 'Supervised Pretrained CNN',
    paper: 'https://arxiv.org/abs/1512.03385',
    paper_pub: SiArxiv,
    code: 'https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html',
    code_name: SiPytorch,
},

// {
//     id: 'GPFM',
//     name: 'GPFM',
//     parameters: 303,
//     dim: 1024,
//     trainingData: '72K Public WSIs, 190M Patches',
//     description: 'The model is pretrained using a modified DINO-v2 framework, distilling knowledge from existing foundation models (UNI, CONCH, and Phikon)',
//     architecture: 'ViT-L',
//     modelType: 'Self and expert knowledge distillation',
//     paper: 'https://arxiv.org/abs/2407.18449',
//     paper_pub: SiArxiv,
//     code: 'https://github.com/birkhoffkiki/GPFM',
//     code_name: FaGithub
// },

// {
//     id: 'CTranspath',
//     name: 'CTranspath',
//     parameters: 27,
//     dim: 1024,
//     trainingData: '32K WSIs (TCGA+PAIP), 4.2M Patches',
//     description: 'The model is pretrained using a modified DINO-v2 framework, distilling knowledge from existing foundation models (UNI, CONCH, and Phikon)',
//     architecture: 'Swin Transformer',
//     modelType: 'MoCoV3',
//     paper: 'https://www.sciencedirect.com/science/article/pii/S1361841522002043',
//     paper_pub: FaJournalWhills,
//     code: 'https://github.com/Xiyue-Wang/TransPath',
//     code_name: FaGithub,
// },

{
    id: 'PLIP',
    name: 'PLIP',
    parameters: 87,
    dim: 512,
    trainingData: 'OpenPath',
    description: 'A Pathology CLIP model',
    architecture: 'ViT-B',
    modelType: 'CLIP',
    paper: 'https://www.nature.com/articles/s41591-023-02504-3',
    paper_pub: FaJournalWhills,
    code: 'https://huggingface.co/spaces/vinid/webplip',
    code_name: SiHuggingface,
},

// {
//     id: 'Phikon',
//     name: 'Phikon',
//     parameters: 86,
//     dim: 512,
//     trainingData: '6K WSIs (TCGA), 43M Patches',
//     description: 'a i-BOT Pretrained model',
//     architecture: 'ViT-B',
//     modelType: 'i-BOT',
//     paper: 'https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2',
//     paper_pub: SiArxiv,
//     code: 'https://huggingface.co/owkin/phikon',
//     code_name: SiHuggingface,
// },

{
    id: 'UNI',
    name: 'UNI',
    parameters: 303,
    dim: 1024,
    trainingData: '100K WSIs (Private +GTex), 100M Patches',
    description: 'a DINOv2 Pretrained model',
    architecture: 'ViT-L',
    modelType: 'DINOv2',
    paper: 'https://huggingface.co/MahmoodLab/UNI',
    paper_pub: FaJournalWhills,
    code: 'https://huggingface.co/MahmoodLab/UNI',
    code_name: SiHuggingface,
},

{
    id: 'CONCH',
    name: 'CONCH',
    parameters: 86,
    dim: 1024,
    trainingData: '1.2M Patches',
    description: '',
    architecture: 'ViT-B',
    modelType: 'CLIP',
    paper: 'https://www.nature.com/articles/s41591-024-02856-4',
    paper_pub: FaJournalWhills,
    code: 'https://huggingface.co/MahmoodLab/CONCH',
    code_name: SiHuggingface,
},

{
    id: 'CHIEF',
    name: 'CHIEF',
    parameters: 27,
    dim: 1024,
    trainingData: '60K WSIs, 15M Patches',
    description: '',
    architecture: 'Swin-Transformer',
    modelType: 'MoCoV3+CLIP',
    paper: 'https://www.nature.com/articles/s41586-024-07894-z',
    paper_pub: FaJournalWhills,
    code: 'https://github.com/hms-dbmi/CHIEF/tree/main',
    code_name: FaGithub,
},

{
    id: 'Prov-GigaPath',
    name: 'Prov-GigaPath',
    parameters: 1100,
    dim: 1024,
    trainingData: '171K Private WSIs, 1300M Patches',
    description: '',
    architecture: 'ViT-g',
    modelType: 'DINOv2+MAE',
    paper: 'https://www.nature.com/articles/s41586-024-07441-w',
    paper_pub: FaJournalWhills,
    code: 'https://github.com/prov-gigapath/prov-gigapath',
    code_name: FaGithub,
},

{
    id: 'mSTAR',
    name: 'mSTAR',
    parameters: 303,
    dim: 1024,
    trainingData: '26,169 Slide-level Modality Pairs over 116M Patches',
    description: '',
    architecture: 'ViT-L/16',
    modelType: 'Self-Taught Training',
    paper: 'https://arxiv.org/abs/2407.15362',
    paper_pub: SiArxiv,
    code: 'https://github.com/Innse/mSTAR',
    code_name: FaGithub,
},

]